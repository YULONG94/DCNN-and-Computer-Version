# 从零开始学习卷积神经网络的笔记（卷积神经网络篇）
我们在之前的文章已经介绍了关于神经网络的基础（假设成立的情况下），而卷积神经网络其实和更加经典的神经网络差不多，最大的差别就是所引入的卷积层，顾名思义，这一层的特色就是做卷积操作，所使用的卷积核自然不能是传统全连接层的那种模式，而是一个多维的线性处理算子（此维度与输入维度相同，否则无法完成卷积操作，对此有疑问的可以先往下看）。
## 卷积神经网络纪年史
+ Kunihiko Fukushima提出的Neocognitron模型[Fukushima and Miyake, 1982]：此神经认知机将一个视觉模式分解成许多子模式（特征），然后进入分层递阶式相连的特征平面进行处理，它试图将视觉系统模型化，使其能够在即使物体有位移或轻微变形的时候，也能完成识别。再往前追溯，可以认为这个模型的推出受到了1959年Hubel和Wiesel提出的感受视野的概念，当时的工作是对猫视觉皮层细胞的研究。
+ Yann Lecun在1998年正式提出了CNN模型（Lenet5），其本质还是一个多层感知机，重点在于所采用的局部连接和权值共享的方式规定了神经网络主要的进行方式。
+ 2012年，Imagenet图像识别大赛中，Hinton组的论文《ImageNet Classification with Deep Convolutional Neural Networks》中提到的Alexnet引入了全新的深层结构和dropout方法，一下子把error rate从25%以上提升到了15%，颠覆了图像识别领域。Alexnet有很多创新点，但现在看来是一项非常简陋的工作。他主要是让人们意识到原来那个福岛邦彦提出，Yann Lecun优化的Lenet结构是有很大改进空间的；只要通过一些方法能够加深这个网络到8层左右，让网络表达能力提升，就能得到出人意料的好结果。
+ 顺着Alexnet的思想，Lecun组2013年提出一个Dropconnect，把error rate提升到了11%。而NUS的颜水成组则提出了Network in Network，NIN的思想是CNN原来的结构是完全可变的，然后加入了一个1*1conv层，NIN的应用也得到了2014年Imagine另一个挑战——图像检测的冠军。Network in Network的思想是CNN结构可以大胆去变化，由此，Inception和VGG在2014年把网络加深到了20层左右，图像识别的error rate也大幅提升到6.7%，接近人类的5.1%。
+ 2015年，MSRA的任少卿、何凯明、孙剑等人，尝试把identity加入到神经网络中。最简单的Identity却出人意料的有效，直接使CNN能够深化到152层、1202层等，error rate也降到了3.6%。后来，ResNeXt, Residual-Attention，DenseNet，SENet等也各有贡献，各自引入了Group convolution，Attention，Dense connection，channelwise-attention等，最终Imagenet上error rate降到了2.2%，完爆人类。现在，即使手机上的神经网络，也能达到超过人类的水平。
+ 而另一个挑战——图像检测中，也是任少卿、何凯明、孙剑等优化了原先的R-CNN, fast R-CNN等通过其他方法提出region proposal,然后用CNN去判断是否是object的方法，提出了faster R-CNN。Faster R-CNN的主要贡献是使用和图像识别相同的CNN feature，发现那个feature不仅可以识别图片是什么东西，还可以用来识别图片在哪个位置！也就是说，CNN的feature非常有用，包含了大量的信息，可以同时用来做不同的task。这个创新一下子把图像检测的MAP也翻倍了。
+ 在短短的4年中，Imagenet图像检测的MAP从最初的0.22达到了最终的0.73。何凯明后来还提出了Mask R-CNN,给faster R-CNN又加了一个mask head。即使只在train中使用mask head，但mask head的信息传递回了原先的CNN feature中，因此使得原先的feature包含更精细的信息。由此，Mask R-CNN得到了更好的结果。
+ CNN结构越来越复杂，于是谷歌提出了Nasnet来自动用Reinforcement Learning 去search一个优化的结构。Nas是目前CV界一个主流的方向，自动寻找出最好的结构，以及给定参数数量/运算量下最好的结构（这样就可以应用于手机），是目前图像识别的发展方向。但何凯明前几天（2019年4月）又发表了一篇论文，表示其实random生成的网络连接结构只要按某些比较好的random方法，都会取得非常好的效果，比标准的好很多。Random和Nas哪个是真的正确的道路，这就有待研究了。
+ 具体的的可以参见一篇比较详细的[博客](https://www.jiqizhixin.com/articles/2019-05-27-4)

## 网络层及特点
### 预处理
+ 平均值归零处理（Mean-subtraction）
> 将所有输入数据从数值上减去平均值，这么做的意义在何处呢，从图片的例子来说，一张照片的像素数值的平均值可以反应场景的整体亮度，但是一般情况下我们不希望我们检测的结果收到这个因素影响，所以这时候我们相当于将所有图片的整体亮度调整到相同。
+ 归一化处理（Normalization）
> 将所有输入数据从树枝上除以标准差，这么做的意义可以这样理解，比如有两个图片，两者的对比图不相同（也就是说，一张照片亮的地方更亮，暗的地方更暗，另一张却相对缓和），那么这时候模型分别在两张图片上训练中的反馈传递（梯度下降）的时候就会出现明显的差异，那么此时就对模型的收敛产生了一些阻碍，如果所有的样本差异都很大，那么在训练的时候，明显会对收敛的速度造成影响。更加恶劣的情况是，如果我们训练采用的样本和测试使用的样本这种差异过大，那么显然会影响我们的模型效果。通过归一化处理我们可以将所有数据的范围压缩到[0, 1]或者[-1, 1],这样的操作不仅仅用在对输入数据的处理上，也可以对后面每一层的输入先进行这样的操作来加速收敛，这就是在卷积网络中经常插入的数据归一化层（batch-normalization）
+ PCA白化（PCA Whitening）
> 其实这是两个过程，第一个是PCA（主成分分析），第二个是白化。这样的操作在神经网络大红之前，似乎风生水起，但是现在的应用似乎越来越难看到（应该是我见识的比较少，才会有这样的想法），主成分分析的作用有两个：一个是数据降维，一个是数据的可视化。在实际应用数据中，样本的维数可能很大，远远大于样本数目，这样模型的复杂度会很大，学习到的模型会过拟合，而且训练速度也会比较慢，内存消耗比较大，但实际数据可能有些维度是线性相关的，可能也含有噪声，这样降维处理就很有必要了，不过PCA对防止过拟合的效果也不明显，一般是根据正则化来防止过拟合。PCA是一种线性降维方法，它通过找出样本空间变化较大的一些正交的坐标方向，可认为就是样本的主成分，然后将样本投影到这些坐标从而降维到一个线性子空间。PCA找出了数据的主成分，丢掉了数据次要的成分，这些次要的成分可能是冗余或者噪声信息。PCA是一种特征选择的方法，假设数据维度为n，我们可以选取前k个方差最大的投影方向，然后把这些数据投影到这个子空间得到k维的数据，得到原数据的“压缩”表示，如果还是全部n个投影方向，我们实际只是选择了与原数据空间坐标轴方向不同的另一组坐标轴方向来表示数据而已。
> 
> 而白化呢，目的就是降低输入的冗余性，更正式的说，我们希望通过白化过程使得学习算法的输入具有如下性质：特征之间相关性较低，所有特征具有相同的方差（也就是方差归一化，这是为了每个特征上的差异相同，也就是每个特征对模型的训练产生的作用大小从理论上达到相同）
> 
> [PCA白化参考](https://blog.csdn.net/walilk/article/details/69660129)
+ 局部归一化处理（Local Contrast Normalization）
> 顾名思义，采取的方式就是对每一个像素点，进行领域内的数据归一化。

### 卷积层
> 终于到了我们最核心的单元之一，毕竟这个项目就是以这个单元的名称冠名的。基本操作呢就是使用一个过滤器（也称卷积核）对输入进行操作，输出称之为特征图。那么什么是Filter呢？假设我们的输入是单通道的一张图片，也就是一个二维的数组，我们所用的卷积核（以下和Filter认为是相同的东西）可以是一个2x2的矩阵，我们知道一个基本的感知器所具备的参数就是权重w和偏置b，那对于一个卷积层来说，所需要训练的参数就是卷积核里面的每个参数（权重），以及一个偏置b，这就是卷积神经网络的一个特点：权值共享，也就是对于这一层所有的神经元，都将采用一套参数进行计算处理（这个过程在卷积操作中我们可以称之为特征提取）关于卷积层的原理解读方面可以看后面的一些优秀博客里面的介绍。

### 池化层
池化层可以认为是对卷积层所产生的的特征图进行简化操作，比如最大值池化，或者均值池化，当感受视野是2x2的时候，最大值池化就是在感受视野内每次只看最大值，均值池化就是每次取感受视野的平均值，池化层的但从设计结构上有两个直接的好处：
+ 降低了下一层待处理的数据量
+ 减少了参数数量，从而可以预防网络过拟合

### 非线性层
卷积神经网络的非线性性主要体现在激活函数上的非线性性，常采用的激活函数比如：sigmoid，tanh等等，具体的形式建议自行百度。
### 全连接层
全连接层其实就是一层感知器神经元，在卷积神经网络中一般用在最后一层作为收尾工作，当然这也是有例外的，比如`NiN`[Lin et al., 2013]就提出将全连接层应用在卷积神经网络中间某层的想法。再比如也有很多全卷积网络（YOLO），从头到尾都是靠卷积层搭建而成。一般来说，卷积神经网络中的全连接层所连接的是一个1x1规模卷积核所产生的特征图，然后两者中的每一个神经元两两之间都有关联，每条关联都包含两个训练参数：权重w和偏置b（貌似我好像说回了感知器的形式，当临时叉题了吧）
### 反卷积层
从名字上来看就可以知道这一层的操作是和卷积层相反，事实也的确如此。如果了解卷积操作的过程（如果没了解过可以看下面优秀博客列表里面的某些，有些博客提供了动态卷积操作的过程），就会明白，卷积操作就是一个卷积核和输入做线性操作，从数学上理解可以将这个过程转为两个矩阵相乘，反卷积想要实现的就是这个逆过程（当然这仅仅是我们这样理解，并不完全正确，实际的开源框架中并不是这样操作的，因为这样处理的过程设计很多零的操作，会浪费时间和内存，这里为了清晰理解这个卷积过程转化为矩阵相乘的操作，还是添加一些书写过程吧，免得觉得我这里东拼西凑的没什么自己的东西，不过这下面的图片有的时候显示不正常，原因的话大概率是网络原因，那么相同的过程我可看我的微信公众号里发的[卷积神经网络]()）。
> 假设我们的输入是一个3x3的矩阵X，卷积核是一个2x2的矩阵W，我们先分别做如下操作：

![输入矩阵转化过程](https://github.com/YULONG94/DCNN-and-Computer-Version/blob/master/imgs/conv_0.png)

> 然后我们先看看实际进行卷积操作时得到的结果：

![正常的计算结果](https://github.com/YULONG94/DCNN-and-Computer-Version/blob/master/imgs/conv_1.png)

> 接下来就是对卷积过程的转化了，我们构造下面的矩阵：

![线性乘法的运算核](https://github.com/YULONG94/DCNN-and-Computer-Version/blob/master/imgs/conv_2.png)

> 再接着我们就可以实现重新换一种形式实现卷积核的操作了：

![卷积核转线性乘法](https://github.com/YULONG94/DCNN-and-Computer-Version/blob/master/imgs/conv_3.png)

上面所书写的过程呢，直接看当然是不可逆的，从数学上也是不可逆的，因为输入9个参数，输出4个参数，线性乘法的参数核是一个9x4的矩阵，相当于信息有损失，如果想达到无损也是有办法的，比如先对输入进行补零的操作，将原本3x3的矩阵扩充成4x4的，此时经过卷积核的操作得到的是3x3的。

+ 补充一点，有人会问怎么填充，是在左边和上边填充零呢还是怎么办，这里我说明一下，一般实际使用的卷积核都是奇数尺寸的，比如3x3或者5x5,此时输入和输出的边长就会相差2或者4，打个比方，当卷积核边长是3，输入是4x4，那么输出就是2x2，那么这时候我们可以选择在输入周围填充一圈零，也就变成了6x6，那么输出就是4x4，这里手动去打太麻烦了，我们可以找来一张纸按照前面的步骤写一遍，可以发现此时就可以将整个过程可逆话，当然证明的话，可以证明此时的矩阵乘法中的参数核存在逆矩阵
+ 第二个补充点，还有人问，卷积核一定是正方形的吗？答案是：否，可以选择边长不相同的卷积核
+ 第三个补充点，那么对于输入输出的尺寸不一样就不可以进行反卷积了吗？答案是：否，以上的过程知识为了说明正常卷积和反卷积都可以看作矩阵的线性乘法，实际使用的时候，一般先将比较小尺寸的输入填充足够多的零，然后做卷积操作，到这里应该突然引起争论，这样解释了一大堆，到头来，说是反卷积，到最后还不是卷积操作。答案是：对啊，所以说前面认为两者是逆过程不合适呢，只不过从一个将输入变小，一个将输入变大，“形象”地认为是相反的过程，而且在有一些网络中也出现过前面是一步一步的卷积操作，而后用反卷积一步一步还原，整个模型呈现一个沙漏的形状

### 感兴趣区域池化层

### 空间金字塔池化
### VLAD特征层
### 空间变化层

## 优秀博客（搬运工就靠这些活着呢）
+ [卷积神经网络结构](https://blog.csdn.net/qq_21578125/article/details/80972996)
+ [卷积神经网络设计理念](https://blog.csdn.net/u010859498/article/details/78794405)
+ [卷积神经网络的计算过程](https://www.cnblogs.com/duanhx/p/9655223.html)
+ [卷积操作的平移不变性](https://www.cnblogs.com/Terrypython/p/11147490.html)
+ [反卷积](https://my.oschina.net/u/3702502/blog/1803358)
+ [卷积神经网络总结](https://blog.csdn.net/jiaoyangwm/article/details/80011656)
+ 着重提一下这个专题里面的所有文章：[吴恩达深度学习笔记](https://www.jianshu.com/u/a2db4e6aa91f)，里面对于理论到应用写的太好了

其实关于卷积操作的其他一些问题也比较意思
+ 旋转不变性：这一点有些争议，因为一般所检测物体旋转的情况是通过增加数据集来使得模型具有比较强的旋转不变性，但是据说也有人设计过[具有旋转不变性的卷积神经网络](https://blog.csdn.net/legend_hua/article/details/81810666)
+ 尺度/光照不变性：由于网络本身就会进行多次采样，所以相同的物体如果在图中的尺寸不一样，那么在某些层他们得到的特征一定是有可比性的，这个比较容易理解。

另外需要补充说明的是几点可能误导人的地方：
+ 卷积操作本身没有各种不变性，而是卷积神经网络具有这种性质，而且这些不变性很大的原因是由于池化层采取的maxpooling的操作原因
+ 卷积神经网络的不变性也不是非常严格的，只是在一定程度上有比较大的鲁棒性，对于检测还是有一些影响，但不代表结果完全一样。