# 从零开始学习卷积神经网络的笔记（卷积神经网络篇）
我们在之前的文章已经介绍了关于神经网络的基础（假设成立的情况下），而卷积神经网络其实和更加经典的神经网络差不多，最大的差别就是所引入的卷积层，顾名思义，这一层的特色就是做卷积操作，所使用的卷积核自然不能是传统全连接层的那种模式，而是一个多维的线性处理算子（此维度与输入维度相同，否则无法完成卷积操作，对此有疑问的可以先往下看）。
## 卷积神经网络纪年史
+ Kunihiko Fukushima提出的Neocognitron模型[Fukushima and Miyake, 1982]：此神经认知机将一个视觉模式分解成许多子模式（特征），然后进入分层递阶式相连的特征平面进行处理，它试图将视觉系统模型化，使其能够在即使物体有位移或轻微变形的时候，也能完成识别。再往前追溯，可以认为这个模型的推出受到了1959年Hubel和Wiesel提出的感受视野的概念，当时的工作是对猫视觉皮层细胞的研究。
+ Yann Lecun在1998年正式提出了CNN模型（Lenet5），其本质还是一个多层感知机，重点在于所采用的局部连接和权值共享的方式规定了神经网络主要的进行方式。
+ 2012年，Imagenet图像识别大赛中，Hinton组的论文《ImageNet Classification with Deep Convolutional Neural Networks》中提到的Alexnet引入了全新的深层结构和dropout方法，一下子把error rate从25%以上提升到了15%，颠覆了图像识别领域。Alexnet有很多创新点，但现在看来是一项非常简陋的工作。他主要是让人们意识到原来那个福岛邦彦提出，Yann Lecun优化的Lenet结构是有很大改进空间的；只要通过一些方法能够加深这个网络到8层左右，让网络表达能力提升，就能得到出人意料的好结果。
+ 顺着Alexnet的思想，Lecun组2013年提出一个Dropconnect，把error rate提升到了11%。而NUS的颜水成组则提出了Network in Network，NIN的思想是CNN原来的结构是完全可变的，然后加入了一个1*1conv层，NIN的应用也得到了2014年Imagine另一个挑战——图像检测的冠军。Network in Network的思想是CNN结构可以大胆去变化，由此，Inception和VGG在2014年把网络加深到了20层左右，图像识别的error rate也大幅提升到6.7%，接近人类的5.1%。
+ 2015年，MSRA的任少卿、何凯明、孙剑等人，尝试把identity加入到神经网络中。最简单的Identity却出人意料的有效，直接使CNN能够深化到152层、1202层等，error rate也降到了3.6%。后来，ResNeXt, Residual-Attention，DenseNet，SENet等也各有贡献，各自引入了Group convolution，Attention，Dense connection，channelwise-attention等，最终Imagenet上error rate降到了2.2%，完爆人类。现在，即使手机上的神经网络，也能达到超过人类的水平。
+ 而另一个挑战——图像检测中，也是任少卿、何凯明、孙剑等优化了原先的R-CNN, fast R-CNN等通过其他方法提出region proposal,然后用CNN去判断是否是object的方法，提出了faster R-CNN。Faster R-CNN的主要贡献是使用和图像识别相同的CNN feature，发现那个feature不仅可以识别图片是什么东西，还可以用来识别图片在哪个位置！也就是说，CNN的feature非常有用，包含了大量的信息，可以同时用来做不同的task。这个创新一下子把图像检测的MAP也翻倍了。
+ 在短短的4年中，Imagenet图像检测的MAP从最初的0.22达到了最终的0.73。何凯明后来还提出了Mask R-CNN,给faster R-CNN又加了一个mask head。即使只在train中使用mask head，但mask head的信息传递回了原先的CNN feature中，因此使得原先的feature包含更精细的信息。由此，Mask R-CNN得到了更好的结果。
+ CNN结构越来越复杂，于是谷歌提出了Nasnet来自动用Reinforcement Learning 去search一个优化的结构。Nas是目前CV界一个主流的方向，自动寻找出最好的结构，以及给定参数数量/运算量下最好的结构（这样就可以应用于手机），是目前图像识别的发展方向。但何凯明前几天（2019年4月）又发表了一篇论文，表示其实random生成的网络连接结构只要按某些比较好的random方法，都会取得非常好的效果，比标准的好很多。Random和Nas哪个是真的正确的道路，这就有待研究了。
+ 具体的的可以参见一篇比较详细的[博客](https://www.jiqizhixin.com/articles/2019-05-27-4)

## 网络层及特点
### 预处理
+ 平均值归零处理（Mean-subtraction）
> 将所有输入数据从数值上减去平均值，这么做的意义在何处呢，从图片的例子来说，一张照片的像素数值的平均值可以反应场景的整体亮度，但是一般情况下我们不希望我们检测的结果收到这个因素影响，所以这时候我们相当于将所有图片的整体亮度调整到相同。
+ 归一化处理（Normalization）
> 将所有输入数据从树枝上除以标准差，这么做的意义可以这样理解，比如有两个图片，两者的对比图不相同（也就是说，一张照片亮的地方更亮，暗的地方更暗，另一张却相对缓和），那么这时候模型分别在两张图片上训练中的反馈传递（梯度下降）的时候就会出现明显的差异，那么此时就对模型的收敛产生了一些阻碍，如果所有的样本差异都很大，那么在训练的时候，明显会对收敛的速度造成影响。更加恶劣的情况是，如果我们训练采用的样本和测试使用的样本这种差异过大，那么显然会影响我们的模型效果。通过归一化处理我们可以将所有数据的范围压缩到[0, 1]或者[-1, 1],这样的操作不仅仅用在对输入数据的处理上，也可以对后面每一层的输入先进行这样的操作来加速收敛，这就是在卷积网络中经常插入的数据归一化层（batch-normalization）
+ PCA白化（PCA Whitening）
> 其实这是两个过程，第一个是PCA（主成分分析），第二个是白化。这样的操作在神经网络大红之前，似乎风生水起，但是现在的应用似乎越来越难看到（应该是我见识的比较少，才会有这样的想法），主成分分析的作用有两个：一个是数据降维，一个是数据的可视化。在实际应用数据中，样本的维数可能很大，远远大于样本数目，这样模型的复杂度会很大，学习到的模型会过拟合，而且训练速度也会比较慢，内存消耗比较大，但实际数据可能有些维度是线性相关的，可能也含有噪声，这样降维处理就很有必要了，不过PCA对防止过拟合的效果也不明显，一般是根据正则化来防止过拟合。PCA是一种线性降维方法，它通过找出样本空间变化较大的一些正交的坐标方向，可认为就是样本的主成分，然后将样本投影到这些坐标从而降维到一个线性子空间。PCA找出了数据的主成分，丢掉了数据次要的成分，这些次要的成分可能是冗余或者噪声信息。PCA是一种特征选择的方法，假设数据维度为n，我们可以选取前k个方差最大的投影方向，然后把这些数据投影到这个子空间得到k维的数据，得到原数据的“压缩”表示，如果还是全部n个投影方向，我们实际只是选择了与原数据空间坐标轴方向不同的另一组坐标轴方向来表示数据而已。
> 
> 而白化呢，目的就是降低输入的冗余性，更正式的说，我们希望通过白化过程使得学习算法的输入具有如下性质：特征之间相关性较低，所有特征具有相同的方差（也就是方差归一化，这是为了每个特征上的差异相同，也就是每个特征对模型的训练产生的作用大小从理论上达到相同）
> 
> [PCA白化参考](https://blog.csdn.net/walilk/article/details/69660129)
+ 局部归一化处理（Local Contrast Normalization）
> 顾名思义，采取的方式就是对每一个像素点，进行领域内的数据归一化。

### 卷积层
> 终于到了我们最核心的单元之一，毕竟这个项目就是以这个单元的名称冠名的。基本操作呢就是使用一个过滤器（也称卷积核）对输入进行操作，输出称之为特征图。那么什么是Filter呢？假设我们的输入是单通道的一张图片，也就是一个二维的数组，我们所用的卷积核（以下和Filter认为是相同的东西）可以是一个2x2的矩阵，我们知道一个基本的感知器所具备的参数就是权重w和偏置b，那对于一个卷积层来说，所需要训练的参数就是卷积核里面的每个参数（权重），以及一个偏置b，这就是卷积神经网络的一个特点：权值共享，也就是对于这一层所有的神经元，都将采用一套参数进行计算处理（这个过程在卷积操作中我们可以称之为特征提取）
> 
> 关于卷积操作的细节有很多优秀博客，我们没必要重复，直接搬运过来
+ [卷积神经网络结构](https://blog.csdn.net/qq_21578125/article/details/80972996)
+ [卷积神经网络设计理念](https://blog.csdn.net/u010859498/article/details/78794405)
+ [卷积神经网络的计算过程](https://www.cnblogs.com/duanhx/p/9655223.html)
+ [卷积操作的平移不变性](https://www.cnblogs.com/Terrypython/p/11147490.html)
+ [卷积神经网络总结](https://blog.csdn.net/jiaoyangwm/article/details/80011656)

其实关于卷积操作的其他一些问题也比较意思
+ 旋转不变性：这一点有些争议，因为一般所检测物体旋转的情况是通过增加数据集来使得模型具有比较强的旋转不变性，但是据说也有人设计过[具有旋转不变性的卷积神经网络](https://blog.csdn.net/legend_hua/article/details/81810666)
+ 尺度/光照不变性：由于网络本身就会进行多次采样，所以相同的物体如果在图中的尺寸不一样，那么在某些层他们得到的特征一定是有可比性的，这个比较容易理解。

另外需要补充说明的是几点可能误导人的地方：
+ 卷积操作本身没有各种不变性，而是卷积神经网络具有这种性质，而且这些不变性很大的原因是由于池化层采取的maxpooling的操作原因
+ 卷积神经网络的不变性也不是非常严格的，只是在一定程度上有比较大的鲁棒性，对于检测还是有一些影响，但不代表结果完全一样。

### 池化层

### 非线性
### 全连接层
### 反卷积层
### 感兴趣区域池化层
### 空间金字塔池化
### VLAD特征层
### 空间变化层